{"num": [["0", "1", "2", "3"], ["4", "5"], ["6"], ["7"], ["8", "9"]], "url": ["https://www.youtube.com/embed/d14TUNcbn1k", "https://www.youtube.com/embed/6niqTuYFZLQ", "https://www.youtube.com/embed/h7iBpEHGVNc", "https://www.youtube.com/embed/Keqep_PKrY8", "https://www.youtube.com/embed/bNb2fEVKeEo"], "start_time": [["00:05:49,901", "00:29:24,392", "00:28:31,853", "01:14:28,754"], ["00:00:24,853", "00:01:46,560"], ["00:08:08,791"], ["00:27:34,70"], ["00:53:56,859", "00:04:44,877"]], "end_time": [["00:05:59,139", "00:29:32,760", "00:28:39,933", "00:00:00,000"], ["00:00:37,346", "00:02:03,060"], ["00:08:32,063"], ["00:27:48,848"], ["00:54:19,328", "00:05:09,830"]], "subtitle": [["Okay, so how does backpropagation work?", " Now, we'll make a gradient stepbased on this truncated backpropagation through time.", " In practice, what people do is this, sort of,approximation called truncated backpropagation through time.", " So, next time we'll really start diving into this idea in more detail, and we'll introduce some neural networks,and start talking about backpropagation as well."], [" Okay, so today we're going to talk about backpropagation and neural networks, and so now we're really startingto get to some of the core material in this class.", " And then we'll dive a little bit into the problems that you can actually quite easily understand once you have figured outhow to take gradients and you actually understand what backpropagation does."], [" And so this is work that was in the 1980s, right, and so by 1998 Yann LeCun basically showed the first example of applying backpropagation and gradient-based learning to train convolutional neural networksthat did really well on document recognition."], [" And so the way we do this is basically by doing backpropagation from a particular neuron activation and seeing what in the input will trigger,will give you the highest values of this neuron."], [" So in order to get these gradients, right, we talked about how, what we should use is backpropagation, right, and this is kind of one of the core techniques of, you know, neural networks, is basicallyusing backpropagation to get your gradients, right?", " And the advantage is that once we can express a function using a computational graph, then we can use a technique that we call backpropagation which is going to recursively use the chain rule in order to compute the gradient with respect to every variable in the computational graph,and so we're going to see how this is done."]], "count": [4, 2, 1, 1, 2]}